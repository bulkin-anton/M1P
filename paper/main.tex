% !TEX program = pdflatex
\documentclass{article}
\usepackage{arxiv}

% ------- Encoding / language -------
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[T2A]{fontenc} % Для корректной кириллицы в pdfLaTeX

% ------- Math / figures / tables -------
\usepackage{amsmath,amssymb,mathtools}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\sisetup{detect-all, table-number-alignment=center}
\usepackage{subcaption}
\usepackage{longtable}

% ------- Citations / links -------
\usepackage{natbib}
\usepackage{doi}
\usepackage{url}

% ------- Title / metadata -------
\title{Сравнение современных методов глубокого обучения\\
для автоматической детекции белух в естественной среде}

\author{
  Булкин Антон Павлович \\
	Московский государственный университет имени М. В. Ломоносова \\
        Научный руководитель: Кравцова Ольга Анатольевна \\
 \texttt{bulkin261@gmail.com} \\
  % TODO: Ваша аффилиация
  % Пример:
  % Кафедра \dots, Университет \dots \\
  % Город, Страна \\
  \texttt{<email@domain>} % TODO: ваша почта или удалите строку
}

\date{2025} % при необходимости можно оставить пустым: \date{}

\renewcommand{\shorttitle}{Детекция белух: сравнение современных методов}

% PDF metadata
\hypersetup{
  pdftitle   = {Сравнение современных методов глубокого обучения для автоматической детекции белух в естественной среде},
  pdfauthor  = {Булкин Антон Павлович},
  pdfsubject = {Computer Vision, Object Detection, Wildlife Monitoring},
  pdfkeywords= {детекция объектов, глубокое обучение, белуха, дроны, спутниковые снимки, трекинг, semi-automatic labeling},
  colorlinks = true,
  linkcolor  = {teal!60!black},
  citecolor  = {teal!60!black},
  urlcolor   = {blue!60!black}
}

\begin{document}
\maketitle

\begin{abstract}
В работе исследуются современные архитектуры глубокого обучения для автоматического мониторинга белух — редкого и охраняемого вида, требующего внимательного и регулярного наблюдения. Цель исследования — повысить эффективность мониторинга животных, минимизируя затраты времени и ресурсов на ручную обработку данных, а также создать систему отслеживания взаимодействия животных между собой в рамках одного видеофайла. Предложен комплексный подход: классические решения CV, полуавтоматическая разметка видеоматериалов, обучение и сравнение современных детекторов и трекеров, а также анализ их точности и скорости.
\end{abstract}

\keywords{детекция объектов \and глубокое обучение \and белуха \and дроны \and спутниковые снимки \and трекинг \and полуавтоматическая разметка}

\section{Введение}
Белуха — редкий и охраняемый вид морских млекопитающих, для которого необходимы регулярные и объективные наблюдения в естественной среде обитания. Традиционные подходы — полевые экспедиции, акустический мониторинг, визуальный подсчёт — трудоёмки, подвержены субъективности и ограничены по пространственному и временно́му охвату. Автоматизация на основе методов компьютерного зрения и глубокого обучения позволяет снизить затраты, повысить воспроизводимость и масштабировать мониторинг.

Задача детекции морских млекопитающих в изображениях и видео обычно решается локализацией объектов с ограничивающими рамками и последующим трекингом. Ранние работы использовали классические техники и первые версии одноэтапных и двухэтапных детекторов, показывая базовую применимость, но чувствительность к условиям съёмки и фоновым помехам. Для видео широко применялись связки «детектор + трекер», повышающие устойчивость за счёт межкадровой ассоциации. Современные одноэтапные архитектуры семейства YOLO существенно улучшили соотношение точности и скорости и пригодны для сценариев реального времени. Трансформерные детекторы упрощают инференс, снимая необходимость в постобработке наподобие подавления немаксимумов, и повышают согласованность предсказаний на сложном фоне. Отдельное направление посвящено снижению стоимости аннотирования за счёт полуавтоматической разметки и стратегий активного обучения.

Ключевые трудности домена — малый видимый размер животных в кадре, блики и рябь воды, частичные окклюзии и близость визуально похожих фоновых объектов (лодки, буи, скалы, водоросли). Существенны доменный сдвиг между локациями и условиями съёмки и дисбаланс классов в многоклассовых сценариях. Высокая стоимость ручной аннотации приводит к ограниченным наборам и ошибкам разметки, что снижает переносимость моделей. Наконец, несогласованные протоколы оценки в литературе — разные наборы данных, пороги пересечения, препроцессинг — затрудняют прямое сравнение результатов и выбор решений для практики.

В работе предлагается воспроизводимый бенчмарк детекции белух в реальной морской среде с унифицированным протоколом обучения и оценки. Подготовлены и сопоставлены наборы изображений разного масштаба — 400, 800, 1200 и около 8000 кадров, — где расширение корпуса достигается полуавтоматической разметкой: начальное обучение модели-помощника, автоматическая аннотация неразмеченных кадров и ручная валидация. Рассматриваются одноклассовая и многоклассовая разметки, что позволяет учитывать типичные фоновые объекты на воде.

\section{Обзор литературы / Related works}
Исследования, посвящённые автоматическому мониторингу белух и других морских млекопитающих, активно развиваются в последние годы и охватывают как аэровидеосъёмку, так и анализ спутниковых данных.
Одной из существенных современных работ является статья Alsaidi~\textit{et~al.}~\cite{alsaidi2024beluga_tracking}, где предложена система детекции и трекинга белух на аэровидео с использованием YOLOv7 и алгоритма DeepSORT. Модель показала высокую точность и полноту, а также устойчивость трекинга после постобработки. Аналогичные идеи развиваются в работе Harasyn~\textit{et~al.}~\cite{harasyn2022drone_beluga}, где YOLOv4 и DeepSORT применялись для детекции белух, каяков и лодок в видеопотоках с дронов; достигнута точность около 74\% и полнота 72\%.
Lee~\textit{et~al.}~\cite{lee2021beluga_retinanet} исследовали RetinaNet и Faster~R-CNN на данных залива Камберленд, отметив, что использование тайлинга крупных изображений существенно повышает точность и уменьшает количество ложных срабатываний.

Особое внимание уделяется построению и масштабированию наборов данных.
Araújo~\textit{et~al.}~\cite{araujo2022beluga_sensors} представили датасет \textit{Beluga-5k} с более чем 5500 фотографиями белух и предложили полуавтоматическую разметку. Лучшая модель (YOLOv3-tiny) достигла 97{,}05\%~mAP@0.5, корректно обнаруживая белух даже в условиях сложного фона.
Boulent~\textit{et~al.}~\cite{boulent2023humanloop} предложили интерактивную схему «человек в контуре», при которой нейросеть, обученная на 100 изображениях, достигла 91\% совпадения с экспертами и позволила ускорить аннотацию более чем в пять раз.
Cubaynes и Fretwell~\cite{cubaynes2022whales_dataset} опубликовали набор спутниковых изображений \textit{Whales from Space}, включающий сотни размеченных примеров китов, что обеспечило возможность обучения моделей на данных высокого разрешения.

Использование спутниковых снимков для мониторинга млекопитающих подтверждается рядом работ.
Green~\textit{et~al.}~\cite{green2023gray} применили YOLOv5 для обнаружения серых китов на спутниковых снимках и достигли точности 80–94\% при полноте 84–89\%.
Guirado~\textit{et~al.}~\cite{guirado2019whale_count} предложили каскадный подход, объединяющий классификацию наличия китов и подсчёт особей, что увеличило общую точность на 36\% по сравнению с одноэтапными методами.
Borowicz~\textit{et~al.}~\cite{borowicz2019aerial} показали, что CNN, обученные на аэрофотоснимках, могут быть перенесены на спутниковые данные без значительной потери качества.

В области общих архитектур объектной детекции значительную роль сыграли двухэтапные модели (Faster~R-CNN~\cite{ren2015fasterrcnn}), обеспечившие высокую точность за счёт сети предложений регионов.
Одноэтапные детекторы, начиная с YOLO~\cite{redmon2016yolo} и YOLO9000~\cite{redmon2017yolo9000}, позволили объединить локализацию и классификацию в одном прогоне сети, достигнув 45–60~FPS.
RetinaNet~\cite{lin2017focal} с функцией Focal~Loss улучшил устойчивость к дисбалансу классов и повысил точность при сохранении скорости.
Последующие версии YOLO (в частности, YOLOv7~\cite{wang2022yolov7}) установили новый стандарт по соотношению точности и производительности.

Трансформерные подходы (DETR~\cite{carion2020end}, RT-DETR~\cite{zhao2023rtdetr}) позволили отказаться от постобработки (NMS) и выполнять end-to-end детекцию. RT-DETR показал сопоставимую с YOLO точность при сохранении real-time скорости, что делает его перспективным для видеоаналитики.
Модели открытого словаря (ViLD~\cite{gu2021vild}, GLIP~\cite{li2022glip}, YOLO-World~\cite{cheng2024yoloworld}) расширили возможности детекции на неизвестные классы, включая морские объекты, что особенно важно для практических систем, где структура сцены заранее не фиксирована.

Обзор Tuia~\textit{et~al.}~\cite{tuia2022wildlife} подчёркивает важность машинного обучения для экологического мониторинга. Авторы указывают, что интеграция глубоких моделей с полевыми данными и дроновыми системами позволяет существенно сократить стоимость и повысить масштабируемость наблюдений.
Совокупно, эти исследования формируют основу для разработки универсального бенчмарка детекции белух и показывают актуальность сопоставления современных архитектур на единых данных и метриках.

\section{Постановка задачи}
\label{sec:problem}

\subsection{Алгебраическая и вероятностная структура данных}
Имеется исходный видеокорпус объёмом порядка 450~ГБ (4K) с бортов квадрокоптеров, из которого формируются выборки изображений размером $N\in\{400,800,1200,\sim 8000\}$ с аннотациями в формате COCO. Каждое изображение $x\in\mathcal{X}$ снабжено множеством объектов $Y=\{(b_j,c_j)\}_{j=1}^{M}$, где $b_j=(x,y,w,h)$ — ограничивающая рамка, $c_j\in\mathcal{C}$ — метка класса. Рассматриваются два варианта аннотаций: \textit{one} (однокласс: $\mathcal{C}=\{\texttt{beluga}\}$) и \textit{mlt} (многокласс: $\mathcal{C}=\{\texttt{beacon, beluga, bird, person, rocks, seaweed, ship}\}$). Разбиения $X^{\mathrm{train}},X^{\mathrm{val}},X^{\mathrm{test}}$ фиксируются на уровне изображений (\texttt{split}$\in\{\texttt{train,val,test}\}$). Формально считаем, что $(X,Y)\sim \mathcal{D}$, где $\mathcal{D}$ — неизвестное распределение, а обучающая выборка $\mathcal{S}=\{(x_i, Y_i)\}_{i=1}^{N}$ представляет собой i.i.d. реализацию из $\mathcal{D}$; $Y_i$ — конечное множество пар «рамка–класс» для $x_i$. Для расширенного набора $\sim 8000$ кадров аннотации получены полуавтоматически: начальная ручная разметка $\to$ предобученный детектор $\to$ авторазметка $\to$ ручная верификация.

\subsection{Отображение и вычислительный конвейер}
Цель — построить детектор
\[
f_\theta:\ \mathcal{X}\to 2^{\mathcal{B}\times\mathcal{C}\times[0,1]},\qquad
f_\theta(x)=\{(\hat b_k,\hat c_k,\hat p_k)\}_{k=1}^{\hat M},
\]
который по входному изображению выдаёт множество предсказанных боксов, классов и оценок уверенности. На практике $f_\theta$ реализуется как композиция стадий
\[
f_\theta \;=\; \underbrace{\pi_{\text{post}}}_{\text{постобработка }} \circ\ 
\underbrace{d_\theta}_{\text{нейросетевой детектор}}\circ\ 
\underbrace{\phi}_{\text{препроцессинг}},
\]
где $\phi$ приводит изображение к требуемому масштабу/формату; $d_\theta$ — параметризуемая модель; $\pi_{\text{post}}$ — схема отбора и консолидации предсказаний. Для open-vocabulary варианта конвейер расширяется текстовым энкодером $t(\cdot)$, формируя совместное представление «изображение–текст» для классов. В одноклассовом режиме \textit{one} пространство меток вырождается до $\{\texttt{beluga}\}$, а \textit{mlt} учитывает семантически близкие фоны (лодки/буи/скалы и т.\,п.), снижая ложные срабатывания. Реализация конвейера, структура датасетов и сценарии \textit{one/mlt} подробно заданы в курсовой работе и репозитории проекта.

\subsection{Внешние критерии качества}
Оценка проводится на тестовой части $X^{\mathrm{test}}$ по стандартным метрикам детекции:
\[
\mathrm{mAP@0.5}=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\mathrm{AP}_{c}(\mathrm{IoU}=0.5),\qquad
\mathrm{mAP@[0.5{:}0.95]}=\frac{1}{|\mathcal{C}|\cdot 10}\sum_{c\in\mathcal{C}}\sum_{\alpha=0.5}^{0.95}\mathrm{AP}_{c}(\mathrm{IoU}=\alpha),
\]
шаг по $\alpha$ равен $0.05$. Дополнительно фиксируются показатели производительности: средняя задержка инференса на кадр $\tau_{\text{inf}}$ (мс/кадр) и при необходимости среднее время одной эпохи обучения $\tau_{\text{train}}$ (мин/эпоха). Для сопоставимости конфигурации препроцессинга, пороги уверенности, параметры NMS (если применимо) и разметочные режимы \textit{one/mlt} унифицированы по моделям и наборам $N$.

\subsection{Оптимизационная постановка}
Параметры $\theta$ оцениваются методом минимизации эмпирического риска на обучающем множестве с использованием детектор-специфичной функции потерь:
\[
\hat\theta \;\in\; \arg\min_{\theta}\ \frac{1}{N}\sum_{i=1}^{N}\ \mathcal{L}\bigl(d_\theta(\phi(x_i)),\, Y_i\bigr)\;+\;\lambda\,\mathcal{R}(\theta),
\]
где $\mathcal{L}$ включает классификационную компоненту и регрессию боксов, $\mathcal{R}$ — регуляризация, $\lambda\ge 0$. С учётом практических ограничений возможна многокритериальная постановка с ограничением на задержку инференса:
\[
\text{minimize } -\mathrm{mAP}@0.5(\theta)\quad \text{s.t.}\quad \tau_{\text{inf}}(\theta)\le \tau^\star,
\]
или эквивалентно — скаляризация качества и скорости:
\[
\min_{\theta}\ \Bigl[-\,\mathrm{mAP}@0.5(\theta)\;+\;\mu\,\tau_{\text{inf}}(\theta)\Bigr], \qquad \mu\ge 0,
\]
что отражает прикладной компромисс «точность/реальное время» для мониторинга с БПЛА и береговых станций. Принятые в работе метрики и протоколы обучения/валидации/теста, а также конкретные наборы $N$ описаны в курсовой и используются далее при сравнении моделей.


\bibliographystyle{unsrtnat}
\bibliography{refs} % ваш .bib файл

\end{document}
