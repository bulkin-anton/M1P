\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[T2A]{fontenc}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\sisetup{detect-all, table-number-alignment=center}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{float}

\usepackage{natbib}
\usepackage{doi}
\usepackage{url}

\usepackage{listings}
\usepackage{xcolor}
\lstset{basicstyle=\ttfamily\small, frame=single, columns=fullflexible, breaklines=true}

\title{Сравнение современных методов глубокого обучения\\
для автоматической детекции белух в естественной среде}

\author{
  Булкин Антон Павлович \\
	Московский государственный университет имени М. В. Ломоносова \\
  Научный руководитель: Кравцова Ольга Анатольевна \\
  \texttt{bulkin261@gmail.com} \\
}

\date{2025}

\renewcommand{\shorttitle}{Детекция белух: сравнение современных методов}

\hypersetup{
  pdftitle   = {Сравнение современных методов глубокого обучения для автоматической детекции белух в естественной среде},
  pdfauthor  = {Булкин Антон Павлович},
  pdfsubject = {Computer Vision, Object Detection, Wildlife Monitoring},
  pdfkeywords= {детекция объектов, глубокое обучение, белуха, дроны, спутниковые снимки, трекинг, semi-automatic labeling},
  colorlinks = true,
  linkcolor  = {teal!60!black},
  citecolor  = {teal!60!black},
  urlcolor   = {blue!60!black}
}

\begin{document}
\maketitle

\begin{abstract}
В работе исследуются современные архитектуры глубокого обучения для автоматического мониторинга белух — редкого и охраняемого вида, требующего внимательного и регулярного наблюдения. Цель исследования — повысить эффективность мониторинга животных, минимизируя затраты времени и ресурсов на ручную обработку данных, а также создать систему отслеживания взаимодействия животных между собой в рамках одного видеофайла. Предложен комплексный подход: классические решения CV, полуавтоматическая разметка видеоматериалов, обучение и сравнение современных детекторов и трекеров, а также анализ их точности и скорости.
\end{abstract}

\keywords{детекция объектов \and глубокое обучение \and белуха \and дроны \and спутниковые снимки \and трекинг \and полуавтоматическая разметка}

\section{Введение}
Белуха — редкий и охраняемый вид морских млекопитающих, для которого необходимы регулярные и объективные наблюдения в естественной среде обитания. Традиционные подходы — полевые экспедиции, акустический мониторинг, визуальный подсчёт — трудоёмки, подвержены субъективности и ограничены по пространственному и временно́му охвату. Автоматизация на основе методов компьютерного зрения и глубокого обучения позволяет снизить затраты, повысить воспроизводимость и масштабировать мониторинг.

Задача детекции морских млекопитающих в изображениях и видео обычно решается локализацией объектов с ограничивающими рамками и последующим трекингом. Ранние работы использовали классические техники и первые версии одноэтапных и двухэтапных детекторов, показывая базовую применимость, но чувствительность к условиям съёмки и фоновым помехам. Для видео широко применялись связки «детектор + трекер», повышающие устойчивость за счёт межкадровой ассоциации. Современные одноэтапные архитектуры семейства YOLO существенно улучшили соотношение точности и скорости и пригодны для сценариев реального времени. Трансформерные детекторы упрощают инференс, снимая необходимость в постобработке наподобие подавления немаксимумов, и повышают согласованность предсказаний на сложном фоне. Отдельное направление посвящено снижению стоимости аннотирования за счёт полуавтоматической разметки и стратегий активного обучения.

Ключевые трудности домена — малый видимый размер животных в кадре, блики и рябь воды, частичные окклюзии и близость визуально похожих фоновых объектов (лодки, буи, скалы, водоросли). Существенны доменный сдвиг между локациями и условиями съёмки и дисбаланс классов в многоклассовых сценариях. Высокая стоимость ручной аннотации приводит к ограниченным наборам и ошибкам разметки, что снижает переносимость моделей. Наконец, несогласованные протоколы оценки в литературе — разные наборы данных, пороги пересечения, препроцессинг — затрудняют прямое сравнение результатов и выбор решений для практики.

В работе предлагается воспроизводимый бенчмарк детекции белух в реальной морской среде с унифицированным протоколом обучения и оценки. Подготовлены и сопоставлены наборы изображений разного масштаба — 400, 800, 1200 и около 8000 кадров, — где расширение корпуса достигается полуавтоматической разметкой: начальное обучение модели-помощника, автоматическая аннотация неразмеченных кадров и ручная валидация. Рассматриваются одноклассовая и многоклассовая разметки, что позволяет учитывать типичные фоновые объекты на воде.

\section{Обзор литературы / Related works}
Исследования, посвящённые автоматическому мониторингу белух и других морских млекопитающих, активно развиваются в последние годы и охватывают как аэровидеосъёмку, так и анализ спутниковых данных.
Одной из существенных современных работ является статья Alsaidi~\textit{et~al.}~\cite{alsaidi2024beluga_tracking}, где предложена система детекции и трекинга белух на аэровидео с использованием YOLOv7 и алгоритма DeepSORT. Модель показала высокую точность и полноту, а также устойчивость трекинга после постобработки. Аналогичные идеи развиваются в работе Harasyn~\textit{et~al.}~\cite{harasyn2022drone_beluga}, где YOLOv4 и DeepSORT применялись для детекции белух, каяков и лодок в видеопотоках с дронов; достигнута точность около 74\% и полнота 72\%.
Lee~\textit{et~al.}~\cite{lee2021beluga_retinanet} исследовали RetinaNet и Faster~R-CNN на данных залива Камберленд, отметив, что использование тайлинга крупных изображений существенно повышает точность и уменьшает количество ложных срабатываний.

Особое внимание уделяется построению и масштабированию наборов данных.
Araújo~\textit{et~al.}~\cite{araujo2022beluga_sensors} представили датасет \textit{Beluga-5k} с более чем 5500 фотографиями белух и предложили полуавтоматическую разметку. Лучшая модель (YOLOv3-tiny) достигла 97{,}05\%~mAP@0.5, корректно обнаруживая белух даже в условиях сложного фона.
Boulent~\textit{et~al.}~\cite{boulent2023humanloop} предложили интерактивную схему «человек в контуре», при которой нейросеть, обученная на 100 изображениях, достигла 91\% совпадения с экспертами и позволила ускорить аннотацию более чем в пять раз.
Cubaynes и Fretwell~\cite{cubaynes2022whales_dataset} опубликовали набор спутниковых изображений \textit{Whales from Space}, включающий сотни размеченных примеров китов, что обеспечило возможность обучения моделей на данных высокого разрешения.

Использование спутниковых снимков для мониторинга млекопитающих подтверждается рядом работ.
Green~\textit{et~al.}~\cite{green2023gray} применили YOLOv5 для обнаружения серых китов на спутниковых снимках и достигли точности 80–94\% при полноте 84–89\%.
Guirado~\textit{et~al.}~\cite{guirado2019whale_count} предложили каскадный подход, объединяющий классификацию наличия китов и подсчёт особей, что увеличило общую точность на 36\% по сравнению с одноэтапными методами.
Borowicz~\textit{et~al.}~\cite{borowicz2019aerial} показали, что CNN, обученные на аэрофотоснимках, могут быть перенесены на спутниковые данные без значительной потери качества.

В области общих архитектур объектной детекции значительную роль сыграли двухэтапные модели (Faster~R-CNN~\cite{ren2015fasterrcnn}), обеспечившие высокую точность за счёт сети предложений регионов.
Одноэтапные детекторы, начиная с YOLO~\cite{redmon2016yolo} и YOLO9000~\cite{redmon2017yolo9000}, позволили объединить локализацию и классификацию в одном прогоне сети, достигнув 45–60~FPS.
RetinaNet~\cite{lin2017focal} с функцией Focal~Loss улучшил устойчивость к дисбалансу классов и повысил точность при сохранении скорости.
Последующие версии YOLO (в частности, YOLOv7~\cite{wang2022yolov7}) установили новый стандарт по соотношению точности и производительности.

Трансформерные подходы (DETR~\cite{carion2020end}, RT-DETR~\cite{zhao2023rtdetr}) позволили отказаться от постобработки (NMS) и выполнять end-to-end детекцию. RT-DETR показал сопоставимую с YOLO точность при сохранении real-time скорости, что делает его перспективным для видеоаналитики.
Модели открытого словаря (ViLD~\cite{gu2021vild}, GLIP~\cite{li2022glip}, YOLO-World~\cite{cheng2024yoloworld}) расширили возможности детекции на неизвестные классы, включая морские объекты, что особенно важно для практических систем, где структура сцены заранее не фиксирована.

Обзор Tuia~\textit{et~al.}~\cite{tuia2022wildlife} подчёркивает важность машинного обучения для экологического мониторинга. Авторы указывают, что интеграция глубоких моделей с полевыми данными и дроновыми системами позволяет существенно сократить стоимость и повысить масштабируемость наблюдений.
Совокупно, эти исследования формируют основу для разработки универсального бенчмарка детекции белух и показывают актуальность сопоставления современных архитектур на единых данных и метриках.

\section{Постановка задачи}
\label{sec:problem}

\subsection{Алгебраическая и вероятностная структура данных}
Имеется исходный видеокорпус объёмом порядка 450~ГБ (4K) с бортов квадрокоптеров, из которого формируются выборки изображений размером $N\in\{400,800,1200,\sim 8000\}$ с аннотациями в формате COCO. Каждое изображение $x\in\mathcal{X}$ снабжено множеством объектов $Y=\{(b_j,c_j)\}_{j=1}^{M}$, где $b_j=(x,y,w,h)$ — ограничивающая рамка, $c_j\in\mathcal{C}$ — метка класса. Рассматриваются два варианта аннотаций: \textit{one} (однокласс: $\mathcal{C}=\{\texttt{beluga}\}$) и \textit{mlt} (многокласс: $\mathcal{C}=\{\texttt{beacon, beluga, bird, person, rocks, seaweed, ship}\}$). Разбиения $X^{\mathrm{train}},X^{\mathrm{val}},X^{\mathrm{test}}$ фиксируются на уровне изображений (\texttt{split}$\in\{\texttt{train,val,test}\}$). Формально считаем, что $(X,Y)\sim \mathcal{D}$, где $\mathcal{D}$ — неизвестное распределение, а обучающая выборка $\mathcal{S}=\{(x_i, Y_i)\}_{i=1}^{N}$ представляет собой i.i.d. реализацию из $\mathcal{D}$; $Y_i$ — конечное множество пар «рамка–класс» для $x_i$. Для расширенного набора $\sim 8000$ кадров аннотации получены полуавтоматически: начальная ручная разметка $\to$ предобученный детектор $\to$ авторазметка $\to$ ручная верификация.

\subsection{Отображение и вычислительный конвейер}
Цель — построить детектор
\[
f_\theta:\ \mathcal{X}\to 2^{\mathcal{B}\times\mathcal{C}\times[0,1]},\qquad
f_\theta(x)=\{(\hat b_k,\hat c_k,\hat p_k)\}_{k=1}^{\hat M},
\]
который по входному изображению выдаёт множество предсказанных боксов, классов и оценок уверенности. На практике $f_\theta$ реализуется как композиция стадий
\[
f_\theta \;=\; \underbrace{\pi_{\text{post}}}_{\text{постобработка }} \circ\ 
\underbrace{d_\theta}_{\text{нейросетевой детектор}}\circ\ 
\underbrace{\phi}_{\text{препроцессинг}},
\]
где $\phi$ приводит изображение к требуемому масштабу/формату; $d_\theta$ — параметризуемая модель; $\pi_{\text{post}}$ — схема отбора и консолидации предсказаний. Для open-vocabulary варианта конвейер расширяется текстовым энкодером $t(\cdot)$, формируя совместное представление «изображение–текст» для классов. В одноклассовом режиме \textit{one} пространство меток вырождается до $\{\texttt{beluga}\}$, а \textit{mlt} учитывает семантически близкие фоны (лодки/буи/скалы и т.\,п.), снижая ложные срабатывания. Реализация конвейера, структура датасетов и сценарии \textit{one/mlt} подробно заданы в программной реализации системы и вспомогательных материалах.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{YOLO_1.png}
  \caption{Схематичное устройство одноэтапного детектора семейства YOLO:
  backbone для извлечения признаков, FPN/PAN для многоуровневой агрегации
  и детекционные головы для разных масштабов объектов.}
  \label{fig:yolo-arch}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{YOLO_3.png}
  \caption{Пример бенчмарка YOLOv12 на стандартных датасетах: зависимость качества
  детекции от скорости инференса в сравнении с предыдущими версиями семейства YOLO.}
  \label{fig:yolov12-benchmark}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{FPN.png}
  \caption{Пример пирамиды признаков (Feature Pyramid Network, FPN), используемой
  в современных детекторах для одновременной обработки мелких и крупных объектов.}
  \label{fig:fpn}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{RTDETR.png}
  \caption{Упрощённая архитектура RT-DETR: сверточный backbone, трансформерный
  энкодер–декодер и предсказание ограничивающих рамок.}
  \label{fig:rtdetr-arch}
\end{figure}

\subsection{Внешние критерии качества}
Оценка проводится на тестовой части $X^{\mathrm{test}}$ по стандартным метрикам детекции:
\[
\mathrm{mAP@0.5}=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\mathrm{AP}_{c}(\mathrm{IoU}=0.5),\qquad
\mathrm{mAP@[0.5{:}0.95]}=\frac{1}{|\mathcal{C}|\cdot 10}\sum_{c\in\mathcal{C}}\sum_{\alpha=0.5}^{0.95}\mathrm{AP}_{c}(\mathrm{IoU}=\alpha),
\]
шаг по $\alpha$ равен $0.05$. Дополнительно фиксируются показатели производительности: средняя задержка инференса на кадр $\tau_{\text{inf}}$ (мс/кадр) и при необходимости среднее время одной эпохи обучения $\tau_{\text{train}}$ (мин/эпоха). Разметочные режимы \textit{one/mlt} унифицированы по моделям и наборам $N$.

\subsection{Оптимизационная постановка}
\label{subsec:optimization}

Введём два уровня оптимизации: (i) обучение детекторов объектов по размеченным
кадрам и (ii) подбор конфигурации трекера, использующего выходы детектора на видеопоследовательностях.

\paragraph{Этап 1: обучение детекторов.}

Пусть $\mathcal{S}=\{(x_i, Y_i)\}_{i=1}^{N}$~--- обучающая выборка статичных изображений,
где $x_i\in\mathcal{X}$, а $Y_i=\{(b_{ij},c_{ij})\}_{j=1}^{M_i}$~--- множество размеченных объектов
(см.~раздел~\ref{sec:problem}). Нейросетевой детектор с параметрами $\theta$
задаёт отображение
\[
d_\theta:\ \mathcal{X}\to 2^{\mathcal{B}\times\mathcal{C}\times[0,1]},\qquad
d_\theta(\phi(x_i))=\{(\hat b_{ik}, \hat c_{ik}, \hat p_{ik})\}_{k=1}^{\hat M_i},
\]
где $\phi$ обозначает препроцессинг, а $\hat p_{ik}$~--- оценка уверенности в предсказании
пары «рамка–класс».

Обучение детектора рассматривается как задача минимизации эмпирического риска
с детектор-специфичной функцией потерь:
\begin{equation}
\label{eq:det-erm}
\hat\theta
\in
\arg\min_{\theta}
\Biggl[
\frac{1}{N}\sum_{i=1}^{N}
\mathcal{L}_{\mathrm{det}}\bigl(d_\theta(\phi(x_i)),\, Y_i\bigr)
\;+\;
\lambda\,\mathcal{R}(\theta)
\Biggr],
\end{equation}
где $\mathcal{L}_{\mathrm{det}}$ включает классификационную компоненты
(кросс-энтропия, Focal Loss и~т.\,п.) и регрессию ограничивающих рамок
(например, IoU- или GIoU-потери), $\mathcal{R}$~--- регуляризатор параметров
(весовой декей, нормализация), $\lambda\ge 0$~--- коэффициент регуляризации.

Внешние критерии качества (mAP@0.5, mAP@[0.5{:}0.95]) задаются на тестовой выборке
$X^{\mathrm{test}}$ как
\[
\mathrm{mAP@0.5}(d_\theta)
=
\frac{1}{|\mathcal{C}|}
\sum_{c\in\mathcal{C}}
\mathrm{AP}_c(\mathrm{IoU}=0.5),
\qquad
\mathrm{mAP@[0.5{:}0.95]}(d_\theta)
=
\frac{1}{|\mathcal{C}|\cdot 10}
\sum_{c\in\mathcal{C}}\;
\sum_{\alpha=0.5}^{0.95}
\mathrm{AP}_c(\mathrm{IoU}=\alpha),
\]
где шаг по $\alpha$ равен $0.05$.
Для практических сценариев мониторинга важна также средняя задержка инференса
на кадр $\tau_{\mathrm{inf}}(d_\theta)$.

С учётом ограничений по времени обработки естественно рассматривать многокритериальную постановку
вида
\begin{equation}
\label{eq:det-multiobj-constraint}
\begin{aligned}
&\text{maximize}\quad && \mathrm{mAP@0.5}(d_\theta) \\
&\text{subject to}\quad && \tau_{\mathrm{inf}}(d_\theta)\le \tau^\star,
\end{aligned}
\end{equation}
где $\tau^\star$~--- допустимый порог задержки для работы в режиме близком к реальному времени.
Эквивалентно можно использовать скаляризацию
\begin{equation}
\label{eq:det-multiobj-scalar}
\hat\theta
\in
\arg\min_{\theta}
\Bigl[
-\,\mathrm{mAP@0.5}(d_\theta)
\;+\;
\mu\,\tau_{\mathrm{inf}}(d_\theta)
\Bigr],\qquad \mu\ge 0,
\end{equation}
что позволяет явно контролировать компромисс «точность / скорость».

\paragraph{Этап 2: использование детeкторов в задаче трекинга.}

Для видеопоследовательностей рассматривается корпус
\[
\mathcal{V}=\bigl\{(v^{(k)}, T^{(k)})\bigr\}_{k=1}^{K},
\]
где $v^{(k)}=(x^{(k)}_1,\dots,x^{(k)}_{T_k})$~--- кадры $k$-го видео,
а $T^{(k)}$~--- размеченные траектории объектов (идентификаторы, классы, рамки по кадрам).
Композиция детектора и трекера с параметрами $\varphi$ задаёт отображение
\[
h_{\theta,\varphi}:\ v^{(k)} \mapsto \hat T^{(k)} = g_\varphi\bigl(d_\theta(\phi(v^{(k)}))\bigr),
\]
где $d_\theta$ применяется покадрово, а $g_\varphi$ выполняет ассоциацию детекций
между кадрами.

Качество многообъектного трекинга оценивается, в частности, по метрике MOTA
(Multi-Object Tracking Accuracy):
\begin{equation}
\label{eq:mota-def}
\mathrm{MOTA}(h_{\theta,\varphi})
=
1
-
\frac{
\sum_{k=1}^{K}\,
\sum_{t=1}^{T_k}
\bigl(
\mathrm{FN}_{k,t} + \mathrm{FP}_{k,t} + \mathrm{IDSW}_{k,t}
\bigr)
}{
\sum_{k=1}^{K}\,
\sum_{t=1}^{T_k}
\mathrm{GT}_{k,t}
},
\end{equation}
где $\mathrm{FN}_{k,t}$, $\mathrm{FP}_{k,t}$ и $\mathrm{IDSW}_{k,t}$~--- соответственно число пропусков,
ложноположительных срабатываний и переключений идентификаторов на кадре $t$ в видео $k$,
$\mathrm{GT}_{k,t}$~--- число истинных объектов.

На практике параметры детектора $\hat\theta$ фиксируются с предыдущего этапа,
а трекер выбирается из конечного семейства алгоритмов $\mathcal{A}$
(DeepSORT, ByteTrack, BoT-SORT) с соответствующими гиперпараметрами
$\varphi\in\Phi_a$ для каждого $a\in\mathcal{A}$. Оптимизация сводится к задаче
гиперпараметрического поиска:
\begin{equation}
\label{eq:tracker-selection}
(\hat a,\hat\varphi)
\in
\arg\max_{a\in\mathcal{A},\,\varphi\in\Phi_a}
\Bigl[
\mathrm{MOTA}\bigl(h_{\hat\theta,\varphi}\bigr)
- \nu\,\tau_{\mathrm{inf}}^{\mathrm{trk}}(a,\varphi)
\Bigr],\qquad \nu\ge 0,
\end{equation}
где $\tau_{\mathrm{inf}}^{\mathrm{trk}}(a,\varphi)$~--- дополнительная задержка,
вносимая трекером к времени детекции.
Таким образом, итоговая система оптимизируется в два шага:
сначала по детектору (формулы~\eqref{eq:det-erm}–\eqref{eq:det-multiobj-scalar}),
затем по конфигурации трекера (формула~\eqref{eq:tracker-selection}),
что отражает естественный приоритет качества детекции и последующую адаптацию
под требования к устойчивости траекторий и скорости.

\section{Эксперименты}
\label{sec:experiments}

В этом разделе описаны схема базового эксперимента по сравнению детекторов
на различных объёмах размеченных данных, а также дополнительное улучшение,
связанное с интеграцией детекторов с алгоритмами трекинга. Постановка задачи
и используемые метрики приведены в разделе~\ref{sec:problem}, здесь мы сосредоточимся
на практической реализации и полученных результатах.

\subsection{Базовый эксперимент: сравнение детекторов}
\label{subsec:base-exp}

Базовый эксперимент направлен на сравнение современных архитектур детекторов
при различных объёмах размеченных данных и двух режимах разметки:
\textit{one} (однокласс: только белухи) и \textit{mlt} (многокласс: белухи и фоновые объекты).
Как и в разделе~\ref{sec:problem}, используются четыре поднабора изображений
\(
N\in\{400, 800, 1200, \sim 8000\}
\)
с фиксированными разбиениями \texttt{train/val/test}.

Для этого:
\begin{itemize}
  \item для каждого $N$ и каждого режима разметки (\textit{one/mlt} при $N\in\{400,800,1200\}$,
  а для $\sim 8000$ кадров используется лучший режим для каждой архитектуры) обучаются
  детекторы семейства YOLOv12, YOLOWorld, RT-DETR и RetinaNet, а также классический
  baseline на основе методов Computer Vision;
  \item протокол обучения унифицируется: фиксируются размер входа, пороги уверенности,
  стратегии аугментации и ранней остановки;
  \item для каждого эксперимента измеряются $\mathrm{mAP@0.5}$ и $\mathrm{mAP@[0.5{:}0.95]}$
  на тестовой части, среднее время одной эпохи обучения $\tau_{\mathrm{train}}$ и средняя
  задержка инференса на одно изображение $\tau_{\mathrm{inf}}$.
\end{itemize}

Для последующего анализа также готовятся визуальные примеры работы моделей
на сложных сценах (мелкие объекты, блики, плотная группа белух).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{example_bel.png}
  \caption{Пример работы современных детекторов на кадре.}
  \label{fig:detectors-qualitative}
\end{figure}

Для каждой модели $m$ и каждого объёма $N$ обучение проводится до сходимости
на $X^{\mathrm{train}}_N$ с контролем по $X^{\mathrm{val}}_N$.
Для детекторов YOLOv12, YOLOWorld и RT-DETR используются реализации на основе
библиотеки Ultralytics, а RetinaNet реализуется через Detectron2.
Классический baseline включает последовательность
«предобработка $\to$ генерация кандидатов $\to$ SVM-классификация $\to$ NMS».

Таблицы~\ref{tab:det-400}--\ref{tab:det-8000} суммируют результаты по основным
метрикам для разных объёмов обучающих данных. Параметр \texttt{type} обозначает
режим разметки (\textit{one} или \textit{mlt}).

\begin{table}[H]
  \centering
  \caption{Сравнение моделей по метрикам детекции при $N=400$.}
  \label{tab:det-400}
  \sisetup{table-number-alignment=center,round-mode=places,round-precision=3}
  \begin{tabular}{l l S S S S}
    \toprule
    Модель & type &
    {mAP@0.5} & {mAP@[0.5{:}0.95]} &
    {$\tau_{\mathrm{train}}$ (min)} & {$\tau_{\mathrm{inf}}$ (ms)} \\
    \midrule
    Baseline  & one & 0.070 & 0.030 & 14.9 & 131.0 \\
    YOLOv12   & one & 0.786 & 0.394 &  0.58 &  15.8 \\
    YOLOv12   & mlt & 0.857 & 0.418 &  0.62 &  16.9 \\
    YOLOWorld & one & 0.773 & 0.375 &  1.31 &   8.7 \\
    YOLOWorld & mlt & 0.796 & 0.397 &  1.39 &  17.4 \\
    RT-DETR   & one & 0.497 & 0.218 &  4.50 &  38.5 \\
    RT-DETR   & mlt & 0.548 & 0.264 &  5.70 &  41.2 \\
    RetinaNet & one & 0.416 & 0.195 &  0.52 &   0.12 \\
    RetinaNet & mlt & 0.215 & 0.181 &  0.59 &   0.12 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Сравнение моделей по метрикам детекции при $N=800$.}
  \label{tab:det-800}
  \sisetup{table-number-alignment=center,round-mode=places,round-precision=3}
  \begin{tabular}{l l S S S S}
    \toprule
    Модель & type &
    {mAP@0.5} & {mAP@[0.5{:}0.95]} &
    {$\tau_{\mathrm{train}}$ (min)} & {$\tau_{\mathrm{inf}}$ (ms)} \\
    \midrule
    Baseline  & one & 0.040 & 0.010 & 16.2 & 148.0 \\
    YOLOv12   & one & 0.819 & 0.403 &  1.41 &  15.6 \\
    YOLOv12   & mlt & 0.874 & 0.474 &  1.42 &  15.6 \\
    YOLOWorld & one & 0.798 & 0.457 &  1.37 &   8.7 \\
    YOLOWorld & mlt & 0.822 & 0.468 &  1.41 &  17.4 \\
    RT-DETR   & one & 0.573 & 0.272 &  9.60 &  43.1 \\
    RT-DETR   & mlt & 0.628 & 0.331 & 10.80 &  45.8 \\
    RetinaNet & one & 0.458 & 0.267 &  0.59 &   0.14 \\
    RetinaNet & mlt & 0.294 & 0.278 &  0.63 &   0.14 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Сравнение моделей по метрикам детекции при $N=1200$.}
  \label{tab:det-1200}
  \sisetup{table-number-alignment=center,round-mode=places,round-precision=3}
  \begin{tabular}{l l S S S S}
    \toprule
    Модель & type &
    {mAP@0.5} & {mAP@[0.5{:}0.95]} &
    {$\tau_{\mathrm{train}}$ (min)} & {$\tau_{\mathrm{inf}}$ (ms)} \\
    \midrule
    Baseline  & one & 0.030 & 0.010 & 18.5 & 154.0 \\
    YOLOv12   & one & 0.870 & 0.470 &  2.01 &  16.8 \\
    YOLOv12   & mlt & 0.877 & 0.463 &  1.58 &  15.6 \\
    YOLOWorld & one & 0.812 & 0.512 &  1.45 &   8.7 \\
    YOLOWorld & mlt & 0.841 & 0.584 &  1.47 &  17.4 \\
    RT-DETR   & one & 0.614 & 0.291 & 10.40 &  43.4 \\
    RT-DETR   & mlt & 0.665 & 0.357 & 11.10 &  46.3 \\
    RetinaNet & one & 0.462 & 0.271 &  0.62 &   0.15 \\
    RetinaNet & mlt & 0.305 & 0.284 &  0.65 &   0.15 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Сравнение моделей по метрикам детекции при $N\approx 8000$
  (используется лучший режим разметки для каждой архитектуры).}
  \label{tab:det-8000}
  \sisetup{table-number-alignment=center,round-mode=places,round-precision=3}
  \begin{tabular}{l S S S S}
    \toprule
    Модель &
    {mAP@0.5} & {mAP@[0.5{:}0.95]} &
    {$\tau_{\mathrm{train}}$ (min)} & {$\tau_{\mathrm{inf}}$ (ms)} \\
    \midrule
    YOLOv12   & 0.989 & 0.652 & 13.85 & 17.60 \\
    YOLOWorld & 0.951 & 0.626 & 16.35 & 10.56 \\
    RT-DETR   & 0.894 & 0.508 & 37.40 & 58.60 \\
    RetinaNet & 0.869 & 0.473 &  0.83 &  4.20 \\
    \bottomrule
  \end{tabular}
\end{table}

Из таблиц видно, что при увеличении объёма обучающих данных детекторы семейства YOLO
стабильно улучшают качество. На крупной выборке YOLOv12 демонстрирует наивысший
$\mathrm{mAP@0.5}$ при умеренной задержке инференса, тогда как YOLOWorld обеспечивает
наилучший баланс между точностью и скоростью. RT-DETR выигрывает по согласованности
предсказаний на сложных сценах, но уступает по времени работы, а RetinaNet остаётся
привлекательной альтернативой в сценариях с жёсткими ограничениями по задержке.

\subsection{Предложенное улучшение: интеграция детекторов с трекерами}
\label{subsec:improvement}

На втором этапе исследуется, как выбранные детекторы ведут себя в связке
с современными алгоритмами трекинга. Основная цель~--- получить устойчивые
траектории белух на видеорядe и оценить выигрыш по метрике MOTA при переходе
от базовой конфигурации к улучшенной.

В качестве детектора выбирается модель YOLOv12 в режиме многоклассовой разметки
(\textit{mlt}) на выборке $N\approx 8000$, показавшая наилучшее сочетание
$\mathrm{mAP@0.5}$ и скорости. Поверх детектора рассматриваются три трекера:
DeepSORT, ByteTrack и BoT-SORT. Для всех трекеров используется единый набор детекций, после чего выполняется индивидуальная
ассоциация объектов по кадрам. Важными элементами конвейера являются более мягкий порог
по уверенности для целевого класса \texttt{beluga} по сравнению с фоновыми классами,
отбрасывание очень коротких и низкоуверенных треков и единая схема препроцессинга
и детекции для всех трекеров.

Для оценки трекинга выделяется поднабор видеопоследовательностей с размеченными
траекториями белух (несколько десятков уникальных животных и сотни траекторий).
На этом наборе для каждой пары «детектор + трекер» вычисляются MOTA и вспомогательные
метрики. Таблица~\ref{tab:trackers-mota} иллюстрирует значения MOTA
для тройки трекеров при фиксированном детекторе YOLOv12 (режим \textit{mlt}).

\begin{table}[H]
  \centering
  \caption{Сравнение трекеров по метрике MOTA (выше~--- лучше) при использовании
  детектора YOLOv12 (\textit{mlt}) на тестовом наборе видеопоследовательностей.}
  \label{tab:trackers-mota}
  \sisetup{table-number-alignment=center,round-mode=places,round-precision=3}
  \begin{tabular}{l S S S S}
    \toprule
    Трекер &
    {MOTA} & {IDF1} &
    {IDSW} & {$\tau_{\mathrm{inf}}^{\mathrm{det+trk}}$ (ms/кадр)} \\
    \midrule
    BoT-SORT  & 0.742 & 0.711 & 41 &  32.5 \\
    ByteTrack & 0.789 & 0.770 & 35 &  30.8 \\
    DeepSORT  & 0.826 & 0.812 & 29 &  33.1 \\
    \bottomrule
  \end{tabular}
\end{table}

Видно, что даже при одинаковых детекциях выбор трекера существенно влияет
на итоговую MOTA. DeepSORT демонстрирует приемлемое качество и умеренное число
переключений идентификаторов, ByteTrack улучшает согласованность траекторий
за счёт более агрессивного использования высокоуверенных детекций, а BoT-SORT
обеспечивает наибольшую MOTA за счёт более аккуратного обращения с пересечениями
траекторий и использования дополнительной информации о скорости движения объектов.

На рисунке~\ref{fig:tracks-example} показан пример работы лучшей
конфигурации (YOLOv12~+ DeepSORT) на видеопоследовательности с несколькими белухами,
движущимися в тесной группе.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{belugas_tracking.png}
  \caption{Пример траекторий белух, восстановленных связкой YOLOv12~+ DeepSORT
  на реальной видеопоследовательности. Цветом и индексами обозначены
  устойчиво отслеживаемые объекты.}
  \label{fig:tracks-example}
\end{figure}

\section{Заключение}

В работе рассмотрена задача автоматической детекции и трекинга белух в естественной
морской среде на основе современных методов глубокого обучения. Сформирован
воспроизводимый бенчмарк, включающий несколько масштабов выборки (от 400 до
$\sim 8000$ кадров) и два режима аннотаций (одноклассовый и многоклассовый), а также
единый протокол обучения и оценки по метрикам mAP и временным характеристикам
инференса. Это позволило сопоставить классический конвейер компьютерного зрения
с современными детекторами семейства YOLO, трансформерным RT-DETR и двухэтапной
архитектурой RetinaNet в единой постановке.

Эксперименты показали, что увеличение объёма размеченных данных приводит к
устойчивому росту качества всех рассмотренных моделей, однако темпы улучшения
существенно зависят от архитектуры. На крупной выборке порядка $8000$ кадров
YOLOv12 достигает наивысшего качества детекции (mAP@0.5 $\approx 0.99$) при
умеренной задержке инференса, тогда как YOLOWorld демонстрирует наилучший
компромисс между точностью и скоростью. RT-DETR обеспечивает сопоставимое
качество при более высокой вычислительной стоимости, выигрывая на сложных
сценах с мелкими объектами и фоновыми помехами. RetinaNet уступает по mAP,
но остаётся привлекательной для сценариев с жёсткими требованиями к задержке
благодаря низкому времени обработки кадра. Отдельно отмечено, что переход
к многоклассовой разметке (\textit{mlt}) за счёт явного учёта фоновых объектов
позволяет снизить число ложных срабатываний на воде и в большинстве режимов
даёт прирост mAP по сравнению с одноклассовым вариантом.

Вторая часть работы была посвящена интеграции лучшего детектора с алгоритмами
трекинга. Показано, что выбор трекера существенно влияет на итоговое качество
многообъектного сопровождения: при фиксированном детекторе YOLOv12 (\textit{mlt})
метрика MOTA варьируется в заметном диапазоне, а наилучшее значение достигается
для связки с DeepSORT (MOTA $\approx 0.83$ при умеренном числе переключений
идентификаторов). Конвейеры на основе BoT-SORT и ByteTrack демонстрируют лишь
незначительно более низкие значения MOTA и IDF1, оставаясь практически применимыми
альтернативами в зависимости от требований к устойчивости треков и задержке
обработки.

В целом, полученные результаты подтверждают, что современные одноэтапные детекторы
семейства YOLO, дополненные корректно настроенным трекером, позволяют строить
практически полезные системы мониторинга белух в реальной морской среде с высокой
точностью и приемлемой скоростью. Перспективными направлениями дальнейшей работы
являются масштабирование видеокорпуса и трекинг-датасета, исследование устойчивости
моделей к доменному сдвигу между акваториями и условиями съёмки, а также более тесная
интеграция детекции и трекинга (joint detection–tracking), включающая использование
открыто-словных архитектур и методов активного или полуавтоматического обучения для
дальнейшего снижения стоимости разметки.


\bibliographystyle{unsrtnat}
\bibliography{refs}

\end{document}
